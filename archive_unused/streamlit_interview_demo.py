#!/usr/bin/env python3
"""
Streamlit Interview Demo - Visual Interface
"""

import streamlit as st
import time
from datetime import datetime
from agent import WeaveAgent
from evaluation.evaluators import ResponseQualityEvaluator
from monitoring.monitors import MonitoringDashboard
import plotly.express as px
import pandas as pd

st.set_page_config(
    page_title="W&B Weave Agent Demo", 
    page_icon="ü§ñ",
    layout="wide"
)

def main():
    st.title("üéØ W&B Weave Agent - Technical Interview Demo")
    st.markdown("**Live demonstration of agent with tracing, evaluation, and monitoring**")
    
    # Live timestamp
    st.markdown(f"üïê **Live Demo Time:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Sidebar
    with st.sidebar:
        st.header("üé¨ Demo Controls")
        demo_mode = st.selectbox(
            "Select Demo Section",
            ["Overview", "Tracing Demo", "Evaluation Demo", "Monitoring Demo", "Live Chat"]
        )
        
        st.header("üìä Live Stats")
        if 'demo_stats' not in st.session_state:
            st.session_state.demo_stats = {
                'queries_processed': 0,
                'avg_quality': 0.0,
                'avg_response_time': 0.0
            }
        
        st.metric("Queries Processed", st.session_state.demo_stats['queries_processed'])
        st.metric("Avg Quality", f"{st.session_state.demo_stats['avg_quality']:.2f}")
        st.metric("Avg Response Time", f"{st.session_state.demo_stats['avg_response_time']:.2f}s")
    
    # Main content
    if demo_mode == "Overview":
        show_overview()
    elif demo_mode == "Tracing Demo":
        show_tracing_demo()
    elif demo_mode == "Evaluation Demo":
        show_evaluation_demo()
    elif demo_mode == "Monitoring Demo":
        show_monitoring_demo()
    elif demo_mode == "Live Chat":
        show_live_chat()

def show_overview():
    st.header("üìã Technical Interview Requirements")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("‚úÖ Required Features")
        st.markdown("""
        - **Weave Tracing**: Capture full execution flow
        - **Evaluations**: Quality assessment system
        - **Monitoring**: Performance tracking
        - **Tool Calling**: Real tool integration
        """)
    
    with col2:
        st.subheader("üéØ Demo Highlights")
        st.markdown("""
        - **Live OpenAI Integration**: Real API calls
        - **Real-time Metrics**: Live performance data
        - **Interactive Interface**: Streamlit UI
        - **Production Ready**: Scalable architecture
        """)
    
    if st.button("üöÄ Start Live Demo"):
        st.success("Demo ready! Use sidebar to navigate sections.")

def show_tracing_demo():
    st.header("üîç Weave Tracing Demo")
    st.markdown("**Demonstrating full execution flow capture**")
    
    if st.button("Run Tracing Demo"):
        agent = WeaveAgent(use_mock=False)
        
        with st.spinner("Processing with tracing..."):
            query = f"Calculate {datetime.now().day} * 15"
            result = agent.process(query)
        
        st.success("‚úÖ Tracing Complete!")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("üìä Trace Results")
            st.json({
                "query": result['query'],
                "tools_used": result['selected_tools'],
                "processing_time": f"{result['processing_time']:.2f}s",
                "reasoning": result['reasoning']['reasoning'][:100] + "..."
            })
        
        with col2:
            st.subheader("üí¨ Response")
            st.write(result['response'])
            
            st.subheader("üîß Tools Executed")
            for tool in result['selected_tools']:
                st.badge(tool, type="secondary")

def show_evaluation_demo():
    st.header("üìà Evaluation System Demo")
    st.markdown("**Quality assessment and scoring**")
    
    if st.button("Run Evaluation Demo"):
        agent = WeaveAgent(use_mock=False)
        evaluator = ResponseQualityEvaluator()
        
        test_queries = [
            "Explain machine learning",
            f"What is {50 * 23}?",
            "Search for AI news"
        ]
        
        results = []
        
        progress_bar = st.progress(0)
        
        for i, query in enumerate(test_queries):
            with st.spinner(f"Evaluating query {i+1}..."):
                result = agent.process(query)
                quality = evaluator.evaluate(query, result['response'])
                
                overall_quality = sum(quality.values()) / len(quality)
                
                results.append({
                    'Query': query,
                    'Relevance': quality['relevance'],
                    'Completeness': quality['completeness'],
                    'Clarity': quality['clarity'],
                    'Overall': overall_quality,
                    'Response Time': result['processing_time']
                })
                
                progress_bar.progress((i + 1) / len(test_queries))
        
        st.success("‚úÖ Evaluation Complete!")
        
        # Display results
        df = pd.DataFrame(results)
        st.dataframe(df, use_container_width=True)
        
        # Quality chart
        fig = px.bar(df, x='Query', y=['Relevance', 'Completeness', 'Clarity'], 
                     title="Quality Metrics by Query")
        st.plotly_chart(fig, use_container_width=True)

def show_monitoring_demo():
    st.header("üìä Monitoring Dashboard Demo")
    st.markdown("**Real-time performance tracking**")
    
    if st.button("Generate Monitoring Data"):
        dashboard = MonitoringDashboard()
        agent = WeaveAgent(use_mock=False)
        
        monitoring_data = []
        
        progress_bar = st.progress(0)
        
        for i in range(5):
            query = f"Monitor test {i+1}: {datetime.now().second + i}"
            
            with st.spinner(f"Recording interaction {i+1}..."):
                result = agent.process(query)
                
                quality = {
                    "relevance": 0.8 + (i * 0.03),
                    "completeness": 0.75 + (i * 0.04),
                    "clarity": 0.85 + (i * 0.02)
                }
                
                dashboard.record_agent_interaction(result, quality)
                
                monitoring_data.append({
                    'Interaction': i + 1,
                    'Response Time': result['processing_time'],
                    'Quality Score': sum(quality.values()) / len(quality),
                    'Tools Used': len(result['selected_tools'])
                })
                
                progress_bar.progress((i + 1) / 5)
        
        st.success("‚úÖ Monitoring Data Generated!")
        
        # Display dashboard
        col1, col2 = st.columns(2)
        
        with col1:
            df = pd.DataFrame(monitoring_data)
            
            # Response time chart
            fig1 = px.line(df, x='Interaction', y='Response Time', 
                          title="Response Time Trend")
            st.plotly_chart(fig1, use_container_width=True)
        
        with col2:
            # Quality score chart
            fig2 = px.line(df, x='Interaction', y='Quality Score', 
                          title="Quality Score Trend")
            st.plotly_chart(fig2, use_container_width=True)
        
        # Summary metrics
        summary = dashboard.get_dashboard_summary()
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if summary.get("performance", {}).get("status") != "no_data":
                st.metric("Avg Response Time", 
                         f"{summary['performance']['avg_response_time']:.2f}s")
        
        with col2:
            if summary.get("errors", {}).get("status") != "no_data":
                st.metric("Success Rate", 
                         f"{summary['errors']['success_rate']:.1%}")
        
        with col3:
            if summary.get("quality", {}).get("status") != "no_data":
                st.metric("Avg Quality", 
                         f"{summary['quality']['avg_quality']:.2f}")

def show_live_chat():
    st.header("üéÆ Live Interactive Demo")
    st.markdown("**Real-time agent interaction**")
    
    if 'messages' not in st.session_state:
        st.session_state.messages = []
    
    # Display chat messages
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            if "metadata" in message:
                with st.expander("üìä Details"):
                    st.json(message["metadata"])
    
    # Chat input
    if prompt := st.chat_input("Ask the agent anything..."):
        # Add user message
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Process with agent
        with st.chat_message("assistant"):
            with st.spinner("Processing..."):
                agent = WeaveAgent(use_mock=False)
                evaluator = ResponseQualityEvaluator()
                
                start_time = time.time()
                result = agent.process(prompt)
                processing_time = time.time() - start_time
                
                quality = evaluator.evaluate(prompt, result['response'])
                avg_quality = sum(quality.values()) / len(quality)
                
                st.markdown(result['response'])
                
                # Update stats
                st.session_state.demo_stats['queries_processed'] += 1
                st.session_state.demo_stats['avg_quality'] = (
                    st.session_state.demo_stats['avg_quality'] + avg_quality
                ) / 2
                st.session_state.demo_stats['avg_response_time'] = (
                    st.session_state.demo_stats['avg_response_time'] + processing_time
                ) / 2
                
                # Add assistant message
                metadata = {
                    "processing_time": processing_time,
                    "tools_used": result['selected_tools'],
                    "quality_score": avg_quality,
                    "reasoning": result['reasoning']['reasoning'][:100] + "..."
                }
                
                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": result['response'],
                    "metadata": metadata
                })
                
                st.rerun()

if __name__ == "__main__":
    main()